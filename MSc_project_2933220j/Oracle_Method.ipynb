{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517ee03-b6ae-41bd-980d-604f675602e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e46573b-d736-4325-bf02-55c2dc854786",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc39ee-9621-47ca-82ee-ee366fe41529",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167ba72-8362-48bd-83a4-1331b650183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680745e6-cbe8-4c7e-8e65-ec4f8e7b9833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d52aa59-d42e-4bd4-8086-bb780ffdf1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'sst2'\n",
    "model_name = 'meta-llama/Llama-3.1-8B'\n",
    "token = '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'\n",
    "single_precision = True\n",
    "gpu_id = 0\n",
    "classes = ['negative', 'positive']\n",
    "class_labels = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "# Updated prompt templates\n",
    "prompts = [\n",
    "    \"Given the following text, does the sentiment lean more towards being positive or negative? Analyze the text carefully before answering.\\nText: {}\\nSentiment:\",\n",
    "    \"What is the emotional sentiment conveyed by the following text? Indicate if it reflects a positive or negative sentiment.\\nText: {}\\nSentiment:\",\n",
    "    \"Is the sentiment in this text generally favorable or unfavorable? Please provide your answer based on the tone of the text.\\nText: {}\\nSentiment:\",\n",
    "    \"Does the following sentence express positive or negative opinion?\\nText: {}\\nSentiment:\",\n",
    "    \"Classify the sentiment of the following sentence as either positive or negative.\\nText: {}\\nSentiment:\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43286f89-c9f4-4899-a560-b4a36bc88749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py:3479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2787ffced0ae4e82b0550f76adb25582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "# Set device and seed\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.cuda.set_device(gpu_id)\n",
    "device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "\n",
    "# Load model and tokenizer\n",
    "def load_model_tokenizer(model_name, single_precision, token):\n",
    "    model = LlamaForCausalLM.from_pretrained(model_name,\n",
    "                                             cache_dir=\"cache/\",\n",
    "                                             torch_dtype=torch.float16 if single_precision else torch.float32,\n",
    "                                             use_auth_token=token)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                              cache_dir=\"cache/\",\n",
    "                                              use_auth_token=token,\n",
    "                                              padding_side=\"left\")\n",
    "    tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model_tokenizer(model_name, single_precision, token)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Get the token indices for the class labels \"positive\" and \"negative\"\n",
    "class_idx = {\n",
    "    0: tokenizer.encode(\"negative\", add_special_tokens=False)[0],\n",
    "    1: tokenizer.encode(\"positive\", add_special_tokens=False)[0]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e6dcf8-7a85-4819-9aa9-c29789928a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                            content\n",
      "0      0     no movement , no yuks , not much of anything .\n",
      "1      0  a gob of drivel so sickly sweet , even the eag...\n",
      "2      0  gangs of new york is an unapologetic mess , wh...\n",
      "3      0  we never really feel involved with the story ,...\n",
      "4      1            this is one of polanski 's best films .\n"
     ]
    }
   ],
   "source": [
    "test_file_path = \"data/test.tsv\"\n",
    "\n",
    "# Read the TSV file with the correct delimiter\n",
    "test_data = pd.read_csv(test_file_path, sep='\\t')\n",
    "\n",
    "# Check the data structure\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aca920d-9419-4b45-96cf-1d323269c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_Mexample(sentence,label, prompt_template, maps, curr_prompt, curr_sentence):\n",
    "    # Format the prompt with the review text\n",
    "\n",
    "    prompt_text = prompt_template.format(sentence)\n",
    "\n",
    "    # Encode the prompt and truncate to fit model's max length\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Extract the logits for the last token and apply softmax for binary classification\n",
    "    last_token_logits = logits[:, -1, [class_idx[0], class_idx[1]]]\n",
    "    probs = torch.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "    # Get predicted class (0 = negative, 1 = positive)\n",
    "    predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "    if predicted_class == label:\n",
    "        if maps[curr_sentence]['confidence'] < abs(probs[0][predicted_class].item() - 0.5):\n",
    "            maps[curr_sentence]['confidence'] = abs(probs[0][predicted_class].item() - 0.5)\n",
    "            maps[curr_sentence]['prompt'] = curr_prompt\n",
    "    else :\n",
    "        if maps[curr_sentence]['prompt'] == -1:\n",
    "            maps[curr_sentence]['prompt'] = curr_prompt\n",
    "            maps[curr_sentence]['confidence'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f3b39ff-a747-47bc-8975-9fb54d7e4f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating using prompt: Given the following text, does the sentiment lean more towards being positive or negative? Analyze the text carefully before answering.\n",
      "Text: {}\n",
      "Sentiment:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1821/1821 [01:13<00:00, 24.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating using prompt: What is the emotional sentiment conveyed by the following text? Indicate if it reflects a positive or negative sentiment.\n",
      "Text: {}\n",
      "Sentiment:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1821/1821 [01:13<00:00, 24.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating using prompt: Is the sentiment in this text generally favorable or unfavorable? Please provide your answer based on the tone of the text.\n",
      "Text: {}\n",
      "Sentiment:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1821/1821 [01:13<00:00, 24.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating using prompt: Does the following sentence express positive or negative opinion?\n",
      "Text: {}\n",
      "Sentiment:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1821/1821 [01:12<00:00, 25.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating using prompt: Classify the sentiment of the following sentence as either positive or negative.\n",
      "Text: {}\n",
      "Sentiment:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1821/1821 [01:12<00:00, 25.04it/s]\n"
     ]
    }
   ],
   "source": [
    "maps = {key: {\"confidence\": 0, \"prompt\": -1} for key in range(len(test_data[\"content\"]))}\n",
    "curr_prompt = 0\n",
    "for prompt_template in prompts:\n",
    "    print(f\"Evaluating using prompt: {prompt_template}\")\n",
    "\n",
    "    # all_preds = []\n",
    "    # all_labels = train_data[\"label\"]  # Ground truth labels\n",
    "    curr_sentence = 0\n",
    "    \n",
    "    for sentence in tqdm(test_data[\"content\"]):\n",
    "        classify_Mexample(sentence,test_data[\"label\"][curr_sentence], prompt_template, maps, curr_prompt, curr_sentence) \n",
    "        curr_sentence += 1 \n",
    "        # all_preds.append(pred)\n",
    "    curr_prompt += 1 \n",
    "    # print(\"Evaluation Metrics for the current prompt:\")\n",
    "    # print(classification_report(all_labels, all_preds, target_names=[\"negative\", \"positive\"]))\n",
    "    # print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ff06f53-000b-43c8-96ff-e2cdba962f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidenceMethod(sentence, prompt_template, curr_sentence, maps):\n",
    "    prompt_temp = prompt_template[maps[curr_sentence]['prompt']]\n",
    "    prompt_text = prompt_temp.format(sentence)\n",
    "    \n",
    "    # Encode the prompt and truncate to fit model's max length\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    # Extract the logits for the last token and apply softmax for binary classification\n",
    "    last_token_logits = logits[:, -1, [class_idx[0], class_idx[1]]]\n",
    "    probs = torch.softmax(last_token_logits, dim=-1)\n",
    "    \n",
    "    # Get predicted class (0 = negative, 1 = positive)\n",
    "    predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "    \n",
    "\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8e070da-39c8-4bb4-8212-31793269837d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1821/1821 [01:12<00:00, 24.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics for Oracle method:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9986    0.7851    0.8791       912\n",
      "    positive     0.8225    0.9989    0.9021       909\n",
      "\n",
      "    accuracy                         0.8918      1821\n",
      "   macro avg     0.9105    0.8920    0.8906      1821\n",
      "weighted avg     0.9107    0.8918    0.8906      1821\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_labels = test_data[\"label\"]  # Ground truth labels\n",
    "curr_sentence = 0\n",
    "data = {\n",
    "    \"statement\": [],\n",
    "    \"prompt\": []\n",
    "}\n",
    "\n",
    "for sentence in tqdm(test_data[\"content\"]):\n",
    "    data[\"statement\"].append(sentence)\n",
    "    data[\"prompt\"].append(maps[curr_sentence]['prompt'])\n",
    "    pred = confidenceMethod(sentence, prompts, curr_sentence, maps)\n",
    "    curr_sentence += 1\n",
    "    all_preds.append(pred)\n",
    "print(\"Evaluation Metrics for Oracle method:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"negative\", \"positive\"], digits=4))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5458ec-0f9c-48c2-9621-c5c56dc8fa44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
