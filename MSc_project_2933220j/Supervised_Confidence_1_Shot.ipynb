{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517ee03-b6ae-41bd-980d-604f675602e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e46573b-d736-4325-bf02-55c2dc854786",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc39ee-9621-47ca-82ee-ee366fe41529",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167ba72-8362-48bd-83a4-1331b650183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680745e6-cbe8-4c7e-8e65-ec4f8e7b9833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d52aa59-d42e-4bd4-8086-bb780ffdf1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'meta-llama/Llama-3.1-8B'\n",
    "token = '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'\n",
    "single_precision = True\n",
    "gpu_id = 0\n",
    "classes = ['negative', 'positive']\n",
    "class_labels = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "# Updated prompt templates\n",
    "prompts = [\n",
    "    \"Given the following text, does the sentiment lean more towards being positive or negative? Analyze the text carefully before answering. \\nExample: \\nText: 'if you 've ever entertained the notion of doing what the title of this film implies , what sex with strangers actually shows may put you off the idea forever.' \\nSentiment: Negative \\nNow analyze the following text: \\nText: {} \\nSentiment:\",\n",
    "    \"What is the emotional sentiment conveyed by the following text? Indicate if it reflects a positive or negative sentiment. \\nExample: \\nText: 'if you 've ever entertained the notion of doing what the title of this film implies , what sex with strangers actually shows may put you off the idea forever.' \\nSentiment: Negative \\nNow analyze the following text: \\nText: {}\\nSentiment:\",\n",
    "    \"Is the sentiment in this text generally favorable or unfavorable? Please provide your answer based on the tone of the text. \\nExample: \\nText: 'if you 've ever entertained the notion of doing what the title of this film implies , what sex with strangers actually shows may put you off the idea forever.' \\nSentiment: Negative \\nNow analyze the following text: \\nText: {}\\nSentiment:\",\n",
    "    \"Does the following sentence express positive or negative opinion? \\nExample: \\nText: 'if you 've ever entertained the notion of doing what the title of this film implies , what sex with strangers actually shows may put you off the idea forever.' \\nSentiment: Negative \\nNow analyze the following text: \\nText: {}\\nSentiment:\",\n",
    "    \"Classify the sentiment of the following sentence as either positive or negative. \\nExample: \\nText: 'if you 've ever entertained the notion of doing what the title of this film implies , what sex with strangers actually shows may put you off the idea forever.' \\nSentiment: Negative \\nNow analyze the following text: \\nText: {}\\nSentiment:\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43286f89-c9f4-4899-a560-b4a36bc88749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py:3479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942f5ac38172497da8cbf0ee1f608bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0af597034941ed9f401fef488dea1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f1f1160027459db17a0017dc62a409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef0ef657f2d44588b999aff1d6ee70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362f34f59d9e4652ba03b89108dc0a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261e3dd4ee42409b9a560e2b112aa532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e1566558874ef3a5163bbcd9804ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad7a9e2bc484de6b78f8a5d5caf0884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e903e6b8e53848edb8dac3449beec427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd185595cd94da9af0974952fccaf6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd607afb0c042fa8f78bb7625653589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81389238b9b646bdb99e45f090c28087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "# Set device and seed\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.cuda.set_device(gpu_id)\n",
    "device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "\n",
    "# Load model and tokenizer\n",
    "def load_model_tokenizer(model_name, single_precision, token):\n",
    "    model = LlamaForCausalLM.from_pretrained(model_name,\n",
    "                                             cache_dir=\"cache/\",\n",
    "                                             torch_dtype=torch.float16 if single_precision else torch.float32,\n",
    "                                             use_auth_token=token)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                              cache_dir=\"cache/\",\n",
    "                                              use_auth_token=token,\n",
    "                                              padding_side=\"left\")\n",
    "    tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model_tokenizer(model_name, single_precision, token)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Get the token indices for the class labels \"positive\" and \"negative\"\n",
    "class_idx = {\n",
    "    0: tokenizer.encode(\"negative\", add_special_tokens=False)[0],\n",
    "    1: tokenizer.encode(\"positive\", add_special_tokens=False)[0]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e6dcf8-7a85-4819-9aa9-c29789928a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                            content\n",
      "0      0     no movement , no yuks , not much of anything .\n",
      "1      0  a gob of drivel so sickly sweet , even the eag...\n",
      "2      0  gangs of new york is an unapologetic mess , wh...\n",
      "3      0  we never really feel involved with the story ,...\n",
      "4      1            this is one of polanski 's best films .\n"
     ]
    }
   ],
   "source": [
    "test_file_path = \"data/test.tsv\"\n",
    "\n",
    "# Read the TSV file with the correct delimiter\n",
    "test_data = pd.read_csv(test_file_path, sep='\\t')\n",
    "\n",
    "# Check the data structure\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5371facd-38e6-4735-bfd3-6534849de8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                            content\n",
      "0      0                       one long string of cliches .\n",
      "1      0  if you 've ever entertained the notion of doin...\n",
      "2      0  k-19 exploits our substantial collective fear ...\n",
      "3      0  it 's played in the most straight-faced fashio...\n",
      "4      1  there is a fabric of complex ideas here , and ...\n"
     ]
    }
   ],
   "source": [
    "dev_file_path = \"data/dev.tsv\"\n",
    "\n",
    "# Read the TSV file with the correct delimiter\n",
    "dev_data = pd.read_csv(dev_file_path, sep='\\t')\n",
    "\n",
    "# Check the data structure\n",
    "print(dev_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69147d40-9146-472a-a375-a83ec38ec022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                           sentence\n",
      "0      0        hide new secretions from the parental units\n",
      "1      0                contains no wit , only labored gags\n",
      "2      1  that loves its characters and communicates som...\n",
      "3      0  remains utterly satisfied to remain the same t...\n",
      "4      0  on the worst revenge-of-the-nerds clichés the ...\n"
     ]
    }
   ],
   "source": [
    "train_file_path = \"data/train.tsv\"\n",
    "\n",
    "# Read the TSV file with the correct delimiter\n",
    "train_data = pd.read_csv(train_file_path, sep='\\t')\n",
    "\n",
    "# Check the data structure\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aca920d-9419-4b45-96cf-1d323269c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_Mexample(sentence,label, prompt_template, maps, curr_prompt, curr_sentence):\n",
    "    # Format the prompt with the review text\n",
    "\n",
    "    prompt_text = prompt_template.format(sentence)\n",
    "\n",
    "    # Encode the prompt and truncate to fit model's max length\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\", padding='longest', truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Extract the logits for the last token and apply softmax for binary classification\n",
    "    last_token_logits = logits[:, -1, [class_idx[0], class_idx[1]]]\n",
    "    probs = torch.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "    # Get predicted class (0 = negative, 1 = positive)\n",
    "    predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "    if predicted_class == label:\n",
    "        if maps[curr_sentence]['confidence'] < abs(probs[0][predicted_class].item() - 0.5):\n",
    "            maps[curr_sentence]['confidence'] = abs(probs[0][predicted_class].item() - 0.5)\n",
    "            maps[curr_sentence]['prompt'] = curr_prompt\n",
    "    else :\n",
    "        if maps[curr_sentence]['prompt'] == -1:\n",
    "            maps[curr_sentence]['prompt'] = curr_prompt\n",
    "            maps[curr_sentence]['confidence'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f3b39ff-a747-47bc-8975-9fb54d7e4f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating using prompt: Given the following text, does the sentiment lean more towards being positive or negative? Analyze the text carefully before answering. \n",
      "Example: \n",
      "Text: 'if you 've ever entertained the notion of doing what the title of this film implies , what sex with strangers actually shows may put you off the idea forever.' \n",
      "Sentiment: Negative \n",
      "Now analyze the following text: \n",
      "Text: {} \n",
      "Sentiment:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67349/67349 [53:45<00:00, 20.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating using prompt: What is the emotional sentiment conveyed by the following text? Indicate if it reflects a positive or negative sentiment. \n",
      "Example: \n",
      "Text: 'if you 've ever entertained the notion of doing what the title of this film implies , what sex with strangers actually shows may put you off the idea forever.' \n",
      "Sentiment: Negative \n",
      "Now analyze the following text: \n",
      "Text: {}\n",
      "Sentiment:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67349/67349 [54:07<00:00, 20.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating using prompt: Is the sentiment in this text generally favorable or unfavorable? Please provide your answer based on the tone of the text. \n",
      "Example: \n",
      "Text: 'if you 've ever entertained the notion of doing what the title of this film implies , what sex with strangers actually shows may put you off the idea forever.' \n",
      "Sentiment: Negative \n",
      "Now analyze the following text: \n",
      "Text: {}\n",
      "Sentiment:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67349/67349 [54:14<00:00, 20.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating using prompt: Does the following sentence express positive or negative opinion? \n",
      "Example: \n",
      "Text: 'if you 've ever entertained the notion of doing what the title of this film implies , what sex with strangers actually shows may put you off the idea forever.' \n",
      "Sentiment: Negative \n",
      "Now analyze the following text: \n",
      "Text: {}\n",
      "Sentiment:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67349/67349 [53:06<00:00, 21.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating using prompt: Classify the sentiment of the following sentence as either positive or negative. \n",
      "Example: \n",
      "Text: 'if you 've ever entertained the notion of doing what the title of this film implies , what sex with strangers actually shows may put you off the idea forever.' \n",
      "Sentiment: Negative \n",
      "Now analyze the following text: \n",
      "Text: {}\n",
      "Sentiment:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67349/67349 [53:24<00:00, 21.02it/s]\n"
     ]
    }
   ],
   "source": [
    "maps = {key: {\"confidence\": 0, \"prompt\": -1} for key in range(len(train_data[\"sentence\"]))}\n",
    "curr_prompt = 0\n",
    "for prompt_template in prompts:\n",
    "    print(f\"Evaluating using prompt: {prompt_template}\")\n",
    "\n",
    "    # all_preds = []\n",
    "    # all_labels = train_data[\"label\"]  # Ground truth labels\n",
    "    curr_sentence = 0\n",
    "    \n",
    "    for sentence in tqdm(train_data[\"sentence\"]):\n",
    "        classify_Mexample(sentence,train_data[\"label\"][curr_sentence], prompt_template, maps, curr_prompt, curr_sentence) \n",
    "        curr_sentence += 1 \n",
    "        # all_preds.append(pred)\n",
    "    curr_prompt += 1 \n",
    "    # print(\"Evaluation Metrics for the current prompt:\")\n",
    "    # print(classification_report(all_labels, all_preds, target_names=[\"negative\", \"positive\"]))\n",
    "    # print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ff06f53-000b-43c8-96ff-e2cdba962f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidenceMethod(sentence, prompt_template, curr_sentence, maps):\n",
    "    prompt_temp = prompt_template[maps[curr_sentence]['prompt']]\n",
    "    prompt_text = prompt_temp.format(sentence)\n",
    "    \n",
    "    # Encode the prompt and truncate to fit model's max length\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\", padding='longest', truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    # Extract the logits for the last token and apply softmax for binary classification\n",
    "    last_token_logits = logits[:, -1, [class_idx[0], class_idx[1]]]\n",
    "    probs = torch.softmax(last_token_logits, dim=-1)\n",
    "    \n",
    "    # Get predicted class (0 = negative, 1 = positive)\n",
    "    predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "    \n",
    "\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8e070da-39c8-4bb4-8212-31793269837d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67349/67349 [54:09<00:00, 20.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics for Confidence method:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9980    0.3702    0.5401     29780\n",
      "    positive     0.6669    0.9994    0.8000     37569\n",
      "\n",
      "    accuracy                         0.7212     67349\n",
      "   macro avg     0.8325    0.6848    0.6700     67349\n",
      "weighted avg     0.8133    0.7212    0.6851     67349\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_labels = train_data[\"label\"]  # Ground truth labels\n",
    "curr_sentence = 0\n",
    "data = {\n",
    "    \"statement\": [],\n",
    "    \"prompt\": []\n",
    "}\n",
    "\n",
    "for sentence in tqdm(train_data[\"sentence\"]):\n",
    "    data[\"statement\"].append(sentence)\n",
    "    data[\"prompt\"].append(maps[curr_sentence]['prompt'])\n",
    "    pred = confidenceMethod(sentence, prompts, curr_sentence, maps)\n",
    "    curr_sentence += 1\n",
    "    all_preds.append(pred)\n",
    "print(\"Evaluation Metrics for Confidence method:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"negative\", \"positive\"], digits=4))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ad4428c-2a26-4d31-af4b-ddc210289cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17dbcff4-768a-4297-91ef-6fb457100b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6555308092056422\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.77      0.69      5767\n",
      "           1       0.30      0.00      0.01       697\n",
      "           2       0.60      0.15      0.23      1094\n",
      "           3       0.50      0.00      0.01       297\n",
      "           4       0.69      0.75      0.72      5615\n",
      "\n",
      "    accuracy                           0.66     13470\n",
      "   macro avg       0.54      0.34      0.33     13470\n",
      "weighted avg       0.63      0.66      0.62     13470\n",
      "\n",
      "Statement: 'I think the actor could have done a better job, overall the stroy was good.' => Predicted Prompt: 4\n",
      "Statement: 'The screenplay was done right and it has perfect climax.' => Predicted Prompt: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. Data Preprocessing\n",
    "X = df[\"statement\"]\n",
    "y = df[\"prompt\"].values\n",
    "\n",
    "# Convert text into numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "# 2. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Train Logistic Regression Model\n",
    "Lmodel = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "Lmodel.fit(X_train, y_train)\n",
    "\n",
    "# 4. Evaluate Model\n",
    "y_pred = Lmodel.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# 5. Prediction for New Statements\n",
    "new_statements = [\"I think the actor could have done a better job, overall the stroy was good.\", \"The screenplay was done right and it has perfect climax.\"]\n",
    "new_statements_vectorized = vectorizer.transform(new_statements)\n",
    "predictions = Lmodel.predict(new_statements_vectorized)\n",
    "\n",
    "for statement, pred in zip(new_statements, predictions):\n",
    "    print(f\"Statement: '{statement}' => Predicted Prompt: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "449310b5-078c-4707-8afc-b6fb6b24163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FinalPrediction(sentence, Lmodel):\n",
    "    new_statements_vectorized = vectorizer.transform(sentence)\n",
    "    predictions = Lmodel.predict(new_statements_vectorized)\n",
    "    prompt_template = prompts[predictions[0]]\n",
    "    prompt_text = prompt_template.format(sentence[0])\n",
    "    \n",
    "    # Encode the prompt and truncate to fit model's max length\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\", padding='longest', truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    # Extract the logits for the last token and apply softmax for binary classification\n",
    "    last_token_logits = logits[:, -1, [class_idx[0], class_idx[1]]]\n",
    "    probs = torch.softmax(last_token_logits, dim=-1)\n",
    "    \n",
    "    # Get predicted class (0 = negative, 1 = positive)\n",
    "    predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "    return predicted_class\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "137c539f-ecb1-4b3a-b7e7-5c2525261ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1821/1821 [01:30<00:00, 20.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics for the current prompt:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9938    0.5285    0.6901       912\n",
      "    positive     0.6781    0.9967    0.8071       909\n",
      "\n",
      "    accuracy                         0.7622      1821\n",
      "   macro avg     0.8360    0.7626    0.7486      1821\n",
      "weighted avg     0.8362    0.7622    0.7485      1821\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_labels = test_data[\"label\"]  # Ground truth labels\n",
    "\n",
    "for sentence in tqdm(test_data[\"content\"]):\n",
    "    pred = FinalPrediction([sentence], Lmodel) \n",
    "    all_preds.append(pred)\n",
    "\n",
    "print(\"Evaluation Metrics for the current prompt:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"negative\", \"positive\"], digits=4))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bf44ac-0ed3-469a-9e75-2524a6d0909e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d4bc05-aaa4-4ac3-b046-055606894ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
