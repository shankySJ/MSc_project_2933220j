{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517ee03-b6ae-41bd-980d-604f675602e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e46573b-d736-4325-bf02-55c2dc854786",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc39ee-9621-47ca-82ee-ee366fe41529",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167ba72-8362-48bd-83a4-1331b650183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680745e6-cbe8-4c7e-8e65-ec4f8e7b9833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d52aa59-d42e-4bd4-8086-bb780ffdf1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'meta-llama/Llama-3.1-8B'\n",
    "token = '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'\n",
    "single_precision = True\n",
    "gpu_id = 0\n",
    "classes = ['negative', 'positive']\n",
    "class_labels = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "# Updated prompt templates\n",
    "prompts = [\n",
    "    \"Given the following text, does the sentiment lean more towards being positive or negative? Analyze the text carefully before answering. \\nExample 1: \\nText: 'if you’ve ever entertained the notion of doing what the title of this film implies, what sex with strangers actually shows may put you off the idea forever.' \\nSentiment: Negative \\nExample 2: \\nText: 'it’s played in the most straight-faced fashion, with little humor to lighten things up.' \\nSentiment: Negative \\nExample 3: \\nText: 'although laced with humor and a few fanciful touches, the film is a refreshingly serious look at young women.' \\nSentiment: Positive \\nNow analyze the following text: \\nText: {}\\nSentiment:\",\n",
    "\n",
    "    \"What is the emotional sentiment conveyed by the following text? Indicate if it reflects a positive or negative sentiment. \\nExample 1: \\nText: 'if you’ve ever entertained the notion of doing what the title of this film implies, what sex with strangers actually shows may put you off the idea forever.' \\nSentiment: Negative \\nExample 2: \\nText: 'it’s played in the most straight-faced fashion, with little humor to lighten things up.' \\nSentiment: Negative \\nExample 3: \\nText: 'although laced with humor and a few fanciful touches, the film is a refreshingly serious look at young women.' \\nSentiment: Positive \\nNow analyze the following text: \\nText: {}\\nSentiment:\",\n",
    "\n",
    "    \"Does the sentiment in this text generally lean favorable or unfavorable? Please provide your answer based on the tone of the text. \\nExample 1: \\nText: 'if you’ve ever entertained the notion of doing what the title of this film implies, what sex with strangers actually shows may put you off the idea forever.' \\nSentiment: Negative \\nExample 2: \\nText: 'it’s played in the most straight-faced fashion, with little humor to lighten things up.' \\nSentiment: Negative \\nExample 3: \\nText: 'although laced with humor and a few fanciful touches, the film is a refreshingly serious look at young women.' \\nSentiment: Positive \\nNow analyze the following text: \\nText: {}\\nSentiment:\",\n",
    "\n",
    "    \"Does the following sentence express a positive or negative opinion? \\nExample 1: \\nText: 'if you’ve ever entertained the notion of doing what the title of this film implies, what sex with strangers actually shows may put you off the idea forever.' \\nSentiment: Negative \\nExample 2: \\nText: 'it’s played in the most straight-faced fashion, with little humor to lighten things up.' \\nSentiment: Negative \\nExample 3: \\nText: 'although laced with humor and a few fanciful touches, the film is a refreshingly serious look at young women.' \\nSentiment: Positive \\nNow analyze the following text: \\nText: {}\\nSentiment:\",\n",
    "\n",
    "    \"Classify the sentiment of the following sentence as either positive or negative. \\nExample 1: \\nText: 'if you’ve ever entertained the notion of doing what the title of this film implies, what sex with strangers actually shows may put you off the idea forever.' \\nSentiment: Negative \\nExample 2: \\nText: 'it’s played in the most straight-faced fashion, with little humor to lighten things up.' \\nSentiment: Negative \\nExample 3: \\nText: 'although laced with humor and a few fanciful touches, the film is a refreshingly serious look at young women.' \\nSentiment: Positive \\nNow analyze the following text: \\nText: {}\\nSentiment:\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43286f89-c9f4-4899-a560-b4a36bc88749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py:3479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6369c57fc6422581b9dbd844d5e71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "# Set device and seed\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.cuda.set_device(gpu_id)\n",
    "device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "\n",
    "# Load model and tokenizer\n",
    "def load_model_tokenizer(model_name, single_precision, token):\n",
    "    model = LlamaForCausalLM.from_pretrained(model_name,\n",
    "                                             cache_dir=\"cache/\",\n",
    "                                             torch_dtype=torch.float16 if single_precision else torch.float32,\n",
    "                                             use_auth_token=token)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                              cache_dir=\"cache/\",\n",
    "                                              use_auth_token=token,\n",
    "                                              padding_side=\"left\")\n",
    "    tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model_tokenizer(model_name, single_precision, token)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Get the token indices for the class labels \"positive\" and \"negative\"\n",
    "class_idx = {\n",
    "    0: tokenizer.encode(\"negative\", add_special_tokens=False)[0],\n",
    "    1: tokenizer.encode(\"positive\", add_special_tokens=False)[0]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e6dcf8-7a85-4819-9aa9-c29789928a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                            content\n",
      "0      0     no movement , no yuks , not much of anything .\n",
      "1      0  a gob of drivel so sickly sweet , even the eag...\n",
      "2      0  gangs of new york is an unapologetic mess , wh...\n",
      "3      0  we never really feel involved with the story ,...\n",
      "4      1            this is one of polanski 's best films .\n"
     ]
    }
   ],
   "source": [
    "test_file_path = \"data/test.tsv\"\n",
    "\n",
    "# Read the TSV file with the correct delimiter\n",
    "test_data = pd.read_csv(test_file_path, sep='\\t')\n",
    "\n",
    "# Check the data structure\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5371facd-38e6-4735-bfd3-6534849de8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                            content\n",
      "0      0                       one long string of cliches .\n",
      "1      0  if you 've ever entertained the notion of doin...\n",
      "2      0  k-19 exploits our substantial collective fear ...\n",
      "3      0  it 's played in the most straight-faced fashio...\n",
      "4      1  there is a fabric of complex ideas here , and ...\n"
     ]
    }
   ],
   "source": [
    "dev_file_path = \"data/dev.tsv\"\n",
    "\n",
    "# Read the TSV file with the correct delimiter\n",
    "dev_data = pd.read_csv(dev_file_path, sep='\\t')\n",
    "\n",
    "# Check the data structure\n",
    "print(dev_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69147d40-9146-472a-a375-a83ec38ec022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                           sentence\n",
      "0      0        hide new secretions from the parental units\n",
      "1      0                contains no wit , only labored gags\n",
      "2      1  that loves its characters and communicates som...\n",
      "3      0  remains utterly satisfied to remain the same t...\n",
      "4      0  on the worst revenge-of-the-nerds clichés the ...\n"
     ]
    }
   ],
   "source": [
    "train_file_path = \"data/train.tsv\"\n",
    "\n",
    "# Read the TSV file with the correct delimiter\n",
    "train_data = pd.read_csv(train_file_path, sep='\\t')\n",
    "\n",
    "# Check the data structure\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aca920d-9419-4b45-96cf-1d323269c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_Mexample(sentence,label, prompt_template, maps, curr_prompt, curr_sentence):\n",
    "    # Format the prompt with the review text\n",
    "\n",
    "    prompt_text = prompt_template.format(sentence)\n",
    "\n",
    "    # Encode the prompt and truncate to fit model's max length\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\", padding='longest', truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    # Extract the logits for the last token and apply softmax for binary classification\n",
    "    last_token_logits = logits[:, -1, [class_idx[0], class_idx[1]]]\n",
    "    probs = torch.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "    # Get predicted class (0 = negative, 1 = positive)\n",
    "    predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "    if predicted_class == label:\n",
    "        if maps[curr_sentence]['confidence'] < abs(probs[0][predicted_class].item() - 0.5):\n",
    "            maps[curr_sentence]['confidence'] = abs(probs[0][predicted_class].item() - 0.5)\n",
    "            maps[curr_sentence]['prompt'] = curr_prompt\n",
    "    else :\n",
    "        if maps[curr_sentence]['prompt'] == -1:\n",
    "            maps[curr_sentence]['prompt'] = curr_prompt\n",
    "            maps[curr_sentence]['confidence'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f3b39ff-a747-47bc-8975-9fb54d7e4f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating using prompt: Given the following text, does the sentiment lean more towards being positive or negative? Analyze the text carefully before answering. \n",
      "Example 1: \n",
      "Text: 'if you’ve ever entertained the notion of doing what the title of this film implies, what sex with strangers actually shows may put you off the idea forever.' \n",
      "Sentiment: Negative \n",
      "Example 2: \n",
      "Text: 'it’s played in the most straight-faced fashion, with little humor to lighten things up.' \n",
      "Sentiment: Negative \n",
      "Example 3: \n",
      "Text: 'although laced with humor and a few fanciful touches, the film is a refreshingly serious look at young women.' \n",
      "Sentiment: Positive \n",
      "Now analyze the following text: \n",
      "Text: {}\n",
      "Sentiment:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67349/67349 [1:11:11<00:00, 15.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating using prompt: What is the emotional sentiment conveyed by the following text? Indicate if it reflects a positive or negative sentiment. \n",
      "Example 1: \n",
      "Text: 'if you’ve ever entertained the notion of doing what the title of this film implies, what sex with strangers actually shows may put you off the idea forever.' \n",
      "Sentiment: Negative \n",
      "Example 2: \n",
      "Text: 'it’s played in the most straight-faced fashion, with little humor to lighten things up.' \n",
      "Sentiment: Negative \n",
      "Example 3: \n",
      "Text: 'although laced with humor and a few fanciful touches, the film is a refreshingly serious look at young women.' \n",
      "Sentiment: Positive \n",
      "Now analyze the following text: \n",
      "Text: {}\n",
      "Sentiment:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67349/67349 [1:10:37<00:00, 15.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating using prompt: Does the sentiment in this text generally lean favorable or unfavorable? Please provide your answer based on the tone of the text. \n",
      "Example 1: \n",
      "Text: 'if you’ve ever entertained the notion of doing what the title of this film implies, what sex with strangers actually shows may put you off the idea forever.' \n",
      "Sentiment: Negative \n",
      "Example 2: \n",
      "Text: 'it’s played in the most straight-faced fashion, with little humor to lighten things up.' \n",
      "Sentiment: Negative \n",
      "Example 3: \n",
      "Text: 'although laced with humor and a few fanciful touches, the film is a refreshingly serious look at young women.' \n",
      "Sentiment: Positive \n",
      "Now analyze the following text: \n",
      "Text: {}\n",
      "Sentiment:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67349/67349 [1:10:50<00:00, 15.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating using prompt: Does the following sentence express a positive or negative opinion? \n",
      "Example 1: \n",
      "Text: 'if you’ve ever entertained the notion of doing what the title of this film implies, what sex with strangers actually shows may put you off the idea forever.' \n",
      "Sentiment: Negative \n",
      "Example 2: \n",
      "Text: 'it’s played in the most straight-faced fashion, with little humor to lighten things up.' \n",
      "Sentiment: Negative \n",
      "Example 3: \n",
      "Text: 'although laced with humor and a few fanciful touches, the film is a refreshingly serious look at young women.' \n",
      "Sentiment: Positive \n",
      "Now analyze the following text: \n",
      "Text: {}\n",
      "Sentiment:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67349/67349 [1:10:03<00:00, 16.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating using prompt: Classify the sentiment of the following sentence as either positive or negative. \n",
      "Example 1: \n",
      "Text: 'if you’ve ever entertained the notion of doing what the title of this film implies, what sex with strangers actually shows may put you off the idea forever.' \n",
      "Sentiment: Negative \n",
      "Example 2: \n",
      "Text: 'it’s played in the most straight-faced fashion, with little humor to lighten things up.' \n",
      "Sentiment: Negative \n",
      "Example 3: \n",
      "Text: 'although laced with humor and a few fanciful touches, the film is a refreshingly serious look at young women.' \n",
      "Sentiment: Positive \n",
      "Now analyze the following text: \n",
      "Text: {}\n",
      "Sentiment:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67349/67349 [1:11:49<00:00, 15.63it/s]\n"
     ]
    }
   ],
   "source": [
    "maps = {key: {\"confidence\": 0, \"prompt\": -1} for key in range(len(train_data[\"sentence\"]))}\n",
    "curr_prompt = 0\n",
    "for prompt_template in prompts:\n",
    "    print(f\"Evaluating using prompt: {prompt_template}\")\n",
    "\n",
    "    # all_preds = []\n",
    "    # all_labels = train_data[\"label\"]  # Ground truth labels\n",
    "    curr_sentence = 0\n",
    "    \n",
    "    for sentence in tqdm(train_data[\"sentence\"]):\n",
    "        classify_Mexample(sentence,train_data[\"label\"][curr_sentence], prompt_template, maps, curr_prompt, curr_sentence) \n",
    "        curr_sentence += 1 \n",
    "        # all_preds.append(pred)\n",
    "    curr_prompt += 1 \n",
    "    # print(\"Evaluation Metrics for the current prompt:\")\n",
    "    # print(classification_report(all_labels, all_preds, target_names=[\"negative\", \"positive\"]))\n",
    "    # print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ff06f53-000b-43c8-96ff-e2cdba962f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidenceMethod(sentence, prompt_template, curr_sentence, maps):\n",
    "    prompt_temp = prompt_template[maps[curr_sentence]['prompt']]\n",
    "    prompt_text = prompt_temp.format(sentence)\n",
    "    \n",
    "    # Encode the prompt and truncate to fit model's max length\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\", padding='longest', truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    # Extract the logits for the last token and apply softmax for binary classification\n",
    "    last_token_logits = logits[:, -1, [class_idx[0], class_idx[1]]]\n",
    "    probs = torch.softmax(last_token_logits, dim=-1)\n",
    "    \n",
    "    # Get predicted class (0 = negative, 1 = positive)\n",
    "    predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "    \n",
    "\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8e070da-39c8-4bb4-8212-31793269837d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67349/67349 [00:00<00:00, 2214261.37it/s]\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_labels = train_data[\"label\"]  # Ground truth labels\n",
    "curr_sentence = 0\n",
    "data = {\n",
    "    \"statement\": [],\n",
    "    \"prompt\": []\n",
    "}\n",
    "\n",
    "for sentence in tqdm(train_data[\"sentence\"]):\n",
    "    data[\"statement\"].append(sentence)\n",
    "    data[\"prompt\"].append(maps[curr_sentence]['prompt'])\n",
    "    curr_sentence += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ad4428c-2a26-4d31-af4b-ddc210289cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17dbcff4-768a-4297-91ef-6fb457100b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.651818856718634\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.47      0.51      2087\n",
      "           1       0.56      0.30      0.40      2165\n",
      "           2       0.73      0.84      0.78      3892\n",
      "           3       0.65      0.19      0.30       752\n",
      "           4       0.64      0.82      0.72      4574\n",
      "\n",
      "    accuracy                           0.65     13470\n",
      "   macro avg       0.63      0.52      0.54     13470\n",
      "weighted avg       0.64      0.65      0.63     13470\n",
      "\n",
      "Statement: 'I think the actor could have done a better job, overall the stroy was good.' => Predicted Prompt: 4\n",
      "Statement: 'The screenplay was done right and it has perfect climax.' => Predicted Prompt: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. Data Preprocessing\n",
    "X = df[\"statement\"]\n",
    "y = df[\"prompt\"].values\n",
    "\n",
    "# Convert text into numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "# 2. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Train Logistic Regression Model\n",
    "Lmodel = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "Lmodel.fit(X_train, y_train)\n",
    "\n",
    "# 4. Evaluate Model\n",
    "y_pred = Lmodel.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# 5. Prediction for New Statements\n",
    "new_statements = [\"I think the actor could have done a better job, overall the stroy was good.\", \"The screenplay was done right and it has perfect climax.\"]\n",
    "new_statements_vectorized = vectorizer.transform(new_statements)\n",
    "predictions = Lmodel.predict(new_statements_vectorized)\n",
    "\n",
    "for statement, pred in zip(new_statements, predictions):\n",
    "    print(f\"Statement: '{statement}' => Predicted Prompt: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "449310b5-078c-4707-8afc-b6fb6b24163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FinalPrediction(sentence, Lmodel):\n",
    "    new_statements_vectorized = vectorizer.transform(sentence)\n",
    "    predictions = Lmodel.predict(new_statements_vectorized)\n",
    "    prompt_template = prompts[predictions[0]]\n",
    "    prompt_text = prompt_template.format(sentence[0])\n",
    "    \n",
    "    # Encode the prompt and truncate to fit model's max length\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\", padding='longest', truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    # Extract the logits for the last token and apply softmax for binary classification\n",
    "    last_token_logits = logits[:, -1, [class_idx[0], class_idx[1]]]\n",
    "    probs = torch.softmax(last_token_logits, dim=-1)\n",
    "    \n",
    "    # Get predicted class (0 = negative, 1 = positive)\n",
    "    predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "    return predicted_class\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "137c539f-ecb1-4b3a-b7e7-5c2525261ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1821/1821 [02:03<00:00, 14.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics for the current prompt:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9896    0.7303    0.8404       912\n",
      "    positive     0.7857    0.9923    0.8770       909\n",
      "\n",
      "    accuracy                         0.8611      1821\n",
      "   macro avg     0.8877    0.8613    0.8587      1821\n",
      "weighted avg     0.8878    0.8611    0.8587      1821\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_labels = test_data[\"label\"]  # Ground truth labels\n",
    "\n",
    "for sentence in tqdm(test_data[\"content\"]):\n",
    "    pred = FinalPrediction([sentence], Lmodel) \n",
    "    all_preds.append(pred)\n",
    "\n",
    "print(\"Evaluation Metrics for the current prompt:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"negative\", \"positive\"], digits=4))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bf44ac-0ed3-469a-9e75-2524a6d0909e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d4bc05-aaa4-4ac3-b046-055606894ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
